{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ovJ7hgNr2C9",
      "metadata": {
        "id": "7ovJ7hgNr2C9"
      },
      "source": [
        "# Environment constants"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yqDFRKB1r7QV",
      "metadata": {
        "id": "yqDFRKB1r7QV"
      },
      "source": [
        "Put here the constants necessary to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7z5oyaRRXrh_",
      "metadata": {
        "id": "7z5oyaRRXrh_"
      },
      "outputs": [],
      "source": [
        "HUGGINGFACE_TOKEN=\"\"\n",
        "CLICKHOUSE_USER=\"\"\n",
        "CLICKHOUSE_PASSWORD=\"\n",
        "CLICKHOUSE_HOST=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77zYTmdhr_OV",
      "metadata": {
        "id": "77zYTmdhr_OV"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d131c8df-aa96-470c-97dd-7c5b540be102",
      "metadata": {
        "collapsed": true,
        "id": "d131c8df-aa96-470c-97dd-7c5b540be102"
      },
      "outputs": [],
      "source": [
        "%pip install datasets==3.6.0\n",
        "%pip install huggingface_hub\n",
        "%pip install sentencepiece\n",
        "%pip install --upgrade transformers\n",
        "%pip install ipywidgets\n",
        "%pip install lxml\n",
        "%pip install clickhouse-connect\n",
        "%pip install pymongo\n",
        "%pip install tensorflow\n",
        "%pip install pythorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jbW4FMHnYIta",
      "metadata": {
        "id": "jbW4FMHnYIta"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED) # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xLtyf8EiqB-t",
      "metadata": {
        "id": "xLtyf8EiqB-t"
      },
      "source": [
        "# ClickHouse\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efsmOoZaltSZ",
      "metadata": {
        "id": "efsmOoZaltSZ"
      },
      "outputs": [],
      "source": [
        "import clickhouse_connect\n",
        "\n",
        "client = clickhouse_connect.get_client(\n",
        "        host=CLICKHOUSE_HOST,\n",
        "        user=CLICKHOUSE_USER,\n",
        "        password=CLICKHOUSE_PASSWORD,\n",
        "        secure=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKwF9353sFXL",
      "metadata": {
        "id": "fKwF9353sFXL"
      },
      "source": [
        "Creates the tables in the database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oq5UzV_FT5Jx",
      "metadata": {
        "id": "oq5UzV_FT5Jx"
      },
      "outputs": [],
      "source": [
        "grams_map = {\n",
        "    1: 'unigrams',\n",
        "    2: 'bigrams',\n",
        "    3: 'trigrams'\n",
        "}\n",
        "\n",
        "def table_creation_command(gram: int) -> str:\n",
        "  tokens = [f\"t{i+1}\" for i in range(gram)]\n",
        "  token_types = [f\"{t} UInt16\" for t in tokens]\n",
        "  return f\"\"\"\n",
        "  CREATE TABLE {grams_map[gram]} (\n",
        "    {',\\n'.join(token_types)},\n",
        "    count UInt64\n",
        "  )\n",
        "  ENGINE = SummingMergeTree()\n",
        "  ORDER BY ({','.join(tokens)});\n",
        "  \"\"\"\n",
        "\n",
        "def reset_or_create_table(gram: int) -> str:\n",
        "  client.query(f\"DROP TABLE IF EXISTS {grams_map[gram]}\")\n",
        "  return client.query(table_creation_command(gram))\n",
        "\n",
        "for gram in grams_map:\n",
        "  reset_or_create_table(gram)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HWRjUVwnqKc5",
      "metadata": {
        "id": "HWRjUVwnqKc5"
      },
      "source": [
        "# Load dataset and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnyEiV83qNwd",
      "metadata": {
        "id": "xnyEiV83qNwd"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Log into HF\n",
        "login(token=HUGGINGFACE_TOKEN)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "ds_stream = load_dataset(\"carolina-c4ai/corpus-carolina\", split=\"corpus\", streaming=True, trust_remote_code=True)\n",
        "\n",
        "# Load the SentencePiece tokenizer\n",
        "model_path = \"TucanoBR/ViTucano-1b5-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KPiaskaQtW6V",
      "metadata": {
        "id": "KPiaskaQtW6V"
      },
      "outputs": [],
      "source": [
        "unigrams = {}\n",
        "bigrams = {}\n",
        "trigrams = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UYuYf_nwV0cs",
      "metadata": {
        "id": "UYuYf_nwV0cs"
      },
      "outputs": [],
      "source": [
        "import ctypes\n",
        "import gc\n",
        "\n",
        "def release_ram():\n",
        "    \"\"\"\n",
        "    Forces garbage collection and releases free memory back to the OS.\n",
        "    \"\"\"\n",
        "    # 1. Collect cyclic garbage in Python\n",
        "    gc.collect()\n",
        "\n",
        "    # 2. Force the C memory allocator (glibc) to release memory to the OS\n",
        "    # This works on Linux-based systems like Google Colab\n",
        "    try:\n",
        "        libc = ctypes.CDLL(\"libc.so.6\")\n",
        "        libc.malloc_trim(0)\n",
        "        print(\"RAM released via malloc_trim.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not run malloc_trim: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34287ace-c37c-4338-91d8-894dfca92572",
      "metadata": {
        "id": "34287ace-c37c-4338-91d8-894dfca92572"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "BATCH = 10_000_000\n",
        "\n",
        "def time_computation(func):\n",
        "  start_time = time.time()\n",
        "  func()\n",
        "  end_time = time.time()\n",
        "  return end_time-start_time\n",
        "\n",
        "def flush(counts: dict[tuple[int], int], gram: int, force_release_ram: bool = False):\n",
        "  table = grams_map[gram]\n",
        "  ops = [\n",
        "      [*k, c] for k,c in counts.items()\n",
        "  ]\n",
        "\n",
        "  start_time = time.time()\n",
        "  client.insert(table, ops, column_names=[f\"t{i+1}\" for i in range(len(ops[0])-1)]+['count'])\n",
        "  end_time = time.time()\n",
        "\n",
        "  counts.clear()\n",
        "\n",
        "  if force_release_ram:\n",
        "    release_ram()\n",
        "\n",
        "  print(f\"[{table.upper()}] Flushed in {end_time-start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2YZ7yoOcZpM",
      "metadata": {
        "id": "W2YZ7yoOcZpM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.mkdir(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iU0-1Yf2WXw1",
      "metadata": {
        "id": "iU0-1Yf2WXw1"
      },
      "outputs": [],
      "source": [
        "with_newline = 0\n",
        "amount_tokens = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4E7Cqdl9ZZq1",
      "metadata": {
        "id": "4E7Cqdl9ZZq1"
      },
      "outputs": [],
      "source": [
        "TOTAL_SAMPLES = 2108999 # hard-coded because the dataset does not provide this metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd7e085-0c82-4860-a86f-e1d6a61d343c",
      "metadata": {
        "id": "cdd7e085-0c82-4860-a86f-e1d6a61d343c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "paths = {\n",
        "    'held_out': 'held_out',\n",
        "    'test': 'test',\n",
        "}\n",
        "\n",
        "files = {}\n",
        "\n",
        "for split in paths:\n",
        "  files[split] = '/'.join([\"data\", paths[split]+'.txt'])\n",
        "\n",
        "print(files)\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "with open(files['test'], 'w', encoding = 'utf-8') as test_file:\n",
        "  with open(files['held_out'], 'w', encoding = 'utf-8') as held_out_file:\n",
        "    for record in tqdm(ds_stream, total = TOTAL_SAMPLES):\n",
        "        if cnt > TOTAL_SAMPLES/2:\n",
        "          break\n",
        "\n",
        "        text = record['text']\n",
        "\n",
        "        if '\\n' in text:\n",
        "          with_newline += 1\n",
        "          continue\n",
        "\n",
        "        p = random.random()\n",
        "\n",
        "        if p <= 0.1:\n",
        "          test_file.write(text + '\\n')\n",
        "          continue\n",
        "        elif p <= 0.2:\n",
        "          held_out_file.write(text + '\\n')\n",
        "          continue\n",
        "\n",
        "        encoding = tokenizer.encode(text)\n",
        "        amount_tokens += len(encoding)\n",
        "\n",
        "        for t in encoding:\n",
        "          unigrams[(t,)] = unigrams.get((t,), 0) + 1\n",
        "\n",
        "        for t1,t2 in zip(encoding, encoding[1:]):\n",
        "            bigrams[(t1,t2)] = bigrams.get((t1,t2),0) + 1\n",
        "\n",
        "        for t1,t2,t3 in zip(encoding, encoding[1:], encoding[2:]):\n",
        "            trigrams[(t1,t2,t3)] = trigrams.get((t1,t2,t3), 0) + 1\n",
        "\n",
        "        for counts, gram in [(unigrams, 1), (bigrams, 2), (trigrams, 3)]:\n",
        "          if len(counts) >= BATCH:\n",
        "            flush(counts, gram)\n",
        "\n",
        "        cnt += 1\n",
        "\n",
        "\n",
        "for counts, gram in [(unigrams, 1), (bigrams, 2), (trigrams, 3)]:\n",
        "  flush(counts, gram)\n",
        "\n",
        "print(f\"Found {with_newline} files that contained a newline character\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b_PKr1xGZ3",
      "metadata": {
        "id": "38b_PKr1xGZ3"
      },
      "outputs": [],
      "source": [
        "for counts, gram in [(unigrams, 1), (bigrams, 2), (trigrams, 3)]:\n",
        "  flush(counts, gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yimqRgm-xwgA",
      "metadata": {
        "id": "yimqRgm-xwgA"
      },
      "source": [
        "# Calculate entropy rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJpMym3lyWBz",
      "metadata": {
        "id": "rJpMym3lyWBz"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-KVXuXArnAzq",
      "metadata": {
        "id": "-KVXuXArnAzq"
      },
      "outputs": [],
      "source": [
        "def query_table(gram, key):\n",
        "  where_clause = \" AND \".join([f\"t{i+1}={v}\" for i, v in enumerate(key)])\n",
        "  group_by = \",\".join([f\"t{i+1}\" for i in range(len(key))])\n",
        "  query_result = client.query(f\"SELECT sum(count) FROM {grams_map[gram]} WHERE {where_clause} GROUP BY {group_by}\").result_set\n",
        "  if query_result:\n",
        "    return query_result[0][0]\n",
        "  return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7nBIma-VzPT-",
      "metadata": {
        "id": "7nBIma-VzPT-"
      },
      "outputs": [],
      "source": [
        "def tri_conditional(t1, t2, t3):\n",
        "  freq_t1_t2_t3 = query_table(3, (t1, t2, t3))\n",
        "  freq_t1_t2 = query_table(2, (t1, t2))\n",
        "\n",
        "  return (freq_t1_t2_t3+1)/(freq_t1_t2+tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-f9NQCsO2ZNr",
      "metadata": {
        "id": "-f9NQCsO2ZNr"
      },
      "outputs": [],
      "source": [
        "def query_table_batch(n_gram_size: int, batch_of_tuples: np.ndarray) -> dict:\n",
        "    if len(batch_of_tuples) == 0:\n",
        "        return {}\n",
        "\n",
        "    batch_list = [tuple(x) for x in batch_of_tuples]\n",
        "\n",
        "    unique_batch = list(set(batch_list))\n",
        "\n",
        "    table = grams_map[n_gram_size]\n",
        "    cols = \",\".join([f\"t{i+1}\" for i in range(n_gram_size)])\n",
        "\n",
        "    counts_map = {k: 0 for k in batch_list}\n",
        "\n",
        "    chunk_size = 5000\n",
        "\n",
        "    for i in range(0, len(unique_batch), chunk_size):\n",
        "        chunk = unique_batch[i:i+chunk_size]\n",
        "\n",
        "        values_str = \", \".join(f\"({','.join(map(str, t))})\" for t in chunk)\n",
        "\n",
        "        query = f\"\"\"\n",
        "            SELECT {cols}, sum(count)\n",
        "            FROM {table}\n",
        "            WHERE ({cols}) IN ({values_str})\n",
        "            GROUP BY {cols}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            results = client.query(query).result_set\n",
        "            for row in results:\n",
        "                counts_map[tuple(row[:-1])] = row[-1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk: {e}\")\n",
        "\n",
        "    return counts_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-xZNsd3B48Hr",
      "metadata": {
        "id": "-xZNsd3B48Hr"
      },
      "outputs": [],
      "source": [
        "tot_unigram = client.query(\"SELECT sum(count) from unigrams;\").result_set[0][0]\n",
        "tot_bigram = client.query(\"SELECT sum(count) from bigrams;\").result_set[0][0]\n",
        "print(tot_unigram)\n",
        "print(tot_bigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GieYj_rb_LeJ",
      "metadata": {
        "id": "GieYj_rb_LeJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hceb9aMa_KIu",
      "metadata": {
        "id": "Hceb9aMa_KIu"
      },
      "outputs": [],
      "source": [
        "def log_prob(encoding: list[int]) -> float:\n",
        "  s = query_table(2, encoding[:2])\n",
        "\n",
        "  total_prob = np.log2(s/tot_bigram)\n",
        "\n",
        "  for t1,t2,t3 in zip(encoding, encoding[1:], encoding[2:]):\n",
        "    num = tri_conditional(t1, t2, t3)+1\n",
        "    den = query_table(2, (t1,t2))+tokenizer.vocab_size\n",
        "    total_prob += np.log2(num/den)\n",
        "\n",
        "  return total_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G6JtafRjopNT",
      "metadata": {
        "id": "G6JtafRjopNT"
      },
      "outputs": [],
      "source": [
        "def log_prob_fast(encoding: list[int]) -> float:\n",
        "    n = len(encoding)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "\n",
        "    arr = np.array(encoding)\n",
        "\n",
        "    trigrams_batch = np.column_stack((arr[:-2], arr[1:-1], arr[2:]))\n",
        "    bigrams_batch  = np.column_stack((arr[:-2], arr[1:-1]))\n",
        "\n",
        "    trigrams = query_table_batch(3, trigrams_batch)\n",
        "    bigrams = query_table_batch(2, bigrams_batch)\n",
        "\n",
        "    encoding = tuple(encoding)\n",
        "\n",
        "    t1_first = encoding[0]\n",
        "    count_t1_first = query_table(1, (t1_first,))\n",
        "    log_p_t1 = np.log2(count_t1_first + 1) - np.log2(tot_unigram + tokenizer.vocab_size)\n",
        "    count_t1_t2 = bigrams.get(encoding[:2], 0)\n",
        "    log_p_t2_given_t1 = np.log2(count_t1_t2 + 1) - np.log2(count_t1_first + tokenizer.vocab_size)\n",
        "\n",
        "    numerator = log_p_t1 + log_p_t2_given_t1\n",
        "    denominator = 0\n",
        "\n",
        "    for t1,t2,t3 in zip(encoding, encoding[1:], encoding[2:]):\n",
        "      numerator += np.log2(trigrams[(t1, t2, t3)]+1)\n",
        "      denominator += np.log2(tokenizer.vocab_size+bigrams[(t1,t2)])\n",
        "\n",
        "    return numerator - denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UgQuhipWnXcn",
      "metadata": {
        "id": "UgQuhipWnXcn"
      },
      "outputs": [],
      "source": [
        "with open(files['test'], 'r', encoding = 'utf-8') as test_file:\n",
        "  test_data = test_file.read().split('\\n')\n",
        "\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O18P30kMoKYX",
      "metadata": {
        "id": "O18P30kMoKYX"
      },
      "outputs": [],
      "source": [
        "total_prob = 0\n",
        "total_chars = 0\n",
        "\n",
        "for test_sample in tqdm(test_data, total = len(test_data)):\n",
        "  total_chars += len(test_sample)\n",
        "  encoding = tokenizer.encode(test_sample)\n",
        "\n",
        "  total_prob -= log_prob_fast(encoding)\n",
        "\n",
        "print(f\"Total entropy rate (bits): {total_prob/total_chars}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
