{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWx_43mSy_D0"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "TOKENIZER=\"nicholasKluge/TeenyTinyLlama-460m\"\n",
        "MODEL=\"nicholasKluge/TeenyTinyLlama-460m\"\n",
        "TOTAL_SAMPLES = 2108999 # hard-coded because the dataset does not provide this metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM8_2UHUt3Qk",
        "outputId": "72ba8b86-160f-4047-9ee9-9493cfc1a409"
      },
      "outputs": [],
      "source": [
        "%pip install huggingface_hub\n",
        "%pip install datasets==3.6.0\n",
        "%pip install transformers\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRoeYXvky2xk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFbRm65iuAKa",
        "outputId": "6be9b985-84c8-4927-c705-22f08f40f59c"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, revision='main')\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL, revision='main')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Connected to {device}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei2VroKZyuKI"
      },
      "outputs": [],
      "source": [
        "ds_stream = load_dataset(\"carolina-c4ai/corpus-carolina\", split=\"corpus\", streaming=True, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKaHpq-t5v1w",
        "outputId": "886efcf9-46c7-40c1-d5ea-7dac8ad8e0dc"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"Isto Ã© um teste.\"\n",
        "test_input = tokenizer(test_sentence, return_tensors='pt').to(device)\n",
        "\n",
        "for token in test_input.input_ids[0]:\n",
        "  print(f\"{token} -> {tokenizer.decode(token)}\")\n",
        "\n",
        "print(f\"Number of tokens is {test_input.input_ids.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhOf91gfy0o1",
        "outputId": "7c5a7a62-06ce-4c24-aa4d-59f981ef0c72"
      },
      "outputs": [],
      "source": [
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "cum_nll_nats = 0\n",
        "cum_chars = 0\n",
        "batch_texts = []\n",
        "big_batch_texts = []\n",
        "cnt = 1\n",
        "\n",
        "def process_batch(texts, cum_nll, cum_chr):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length = 4096).to(device)\n",
        "\n",
        "    # Create labels: -100 is ignored by the loss function\n",
        "    labels = inputs.input_ids.clone()\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n",
        "\n",
        "    # outputs.loss is the average loss per valid token in the batch\n",
        "    # We need to scale it back to the total sum of losses\n",
        "    # The model shifts labels internally (labels[..., 1:]), so we count valid tokens in the shifted labels\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "    n_valid_tokens = (shift_labels != -100).sum()\n",
        "\n",
        "    cum_nll += outputs.loss * n_valid_tokens\n",
        "    cum_chr += sum(len(t) for t in texts)\n",
        "    return cum_nll, cum_chr\n",
        "\n",
        "for record in tqdm(ds_stream, total=TOTAL_SAMPLES):\n",
        "    p = random.random()\n",
        "\n",
        "    if p > 0.1:\n",
        "        continue\n",
        "\n",
        "    text = record['text']\n",
        "\n",
        "    batch_texts.append(text)\n",
        "\n",
        "    for batch in [batch_texts]:\n",
        "      if len(batch) >= BATCH_SIZE:\n",
        "          cum_nll_nats, cum_chars = process_batch(batch, cum_nll_nats, cum_chars)\n",
        "          batch.clear()\n",
        "\n",
        "          cnt -= 1\n",
        "          if cnt == 0:\n",
        "            break\n",
        "\n",
        "# Process any remaining samples\n",
        "for batch in [batch_texts]:\n",
        "      if len(batch) >= BATCH_SIZE:\n",
        "          cum_nll_nats, cum_chars = process_batch(batch, cum_nll_nats, cum_chars)\n",
        "          batch.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK14zpDZdU-k"
      },
      "outputs": [],
      "source": [
        "cum_nll_bits = cum_nll_nats / math.log(2)\n",
        "print(f\"Bits per character: {cum_nll_bits / cum_chars}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
